{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IDP Production-Style Orchestration Notebook\n",
        "\n",
        "This notebook simulates the Intelligent Document Processing (IDP) platform in a production-like configuration. It rebuilds sample documents, exercises routing, executes the parsing workflow, applies summarisation, and triggers enrichment using the in-repo Azure Document Intelligence proxy. Cells containing **TODO** markers show exactly where to inject your live credentials, secrets, and downstream integrations when running on Databricks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook goals\n",
        "\n",
        "- Demonstrate the full ingestion \u279c routing \u279c parsing \u279c summarisation \u279c enrichment lifecycle.\n",
        "- Provide production-grade configuration templates for Azure Document Intelligence, Azure OpenAI, and enrichment hooks.\n",
        "- Highlight where to replace simulators with real SNS/SQS, Delta Lake, and Azure SDK clients.\n",
        "- Generate artefacts that mirror what the Databricks jobs persist so you can validate schema and confidence data locally.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the TODO placeholders\n",
        "\n",
        "The notebook runs end-to-end with in-memory simulators. When you are ready to plug into real infrastructure:\n",
        "\n",
        "1. Replace the configuration blocks marked with **TODO** to point at your Azure Document Intelligence endpoint, Azure OpenAI deployment, and enrichment services.\n",
        "2. Swap the simulated queue/storage classes with the production `idp_service.sqs_batch_ingestion` entrypoint.\n",
        "3. Connect to Delta Lake tables instead of the in-memory result store if you want to persist outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Make the repository modules importable\n",
        "\n",
        "Ensure the repository root is discoverable so the notebook can load the `idp_service` package directly without installation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_ROOT = Path.cwd().resolve()\n",
        "if not (REPO_ROOT / \"idp_service\").exists():\n",
        "    REPO_ROOT = REPO_ROOT.parent\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "print(f\"Using repository root: {REPO_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Provide production integration stubs\n",
        "\n",
        "Update the configuration blocks below when connecting to real services. Leaving the placeholders in place keeps the simulation self-contained.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Replace the placeholder values with your Azure Document Intelligence configuration when running in production.\n",
        "# Set USE_LLM_PROXY_FOR_AZURE to False after providing the live credentials below.\n",
        "USE_LLM_PROXY_FOR_AZURE = True\n",
        "\n",
        "AZURE_DOCUMENT_INTELLIGENCE_CONFIG = {\n",
        "    \"endpoint\": \"<FILL-ME: https://<region>.cognitiveservices.azure.com/>\",\n",
        "    \"api_key\": \"<FILL-ME: azure-document-intelligence-key>\",\n",
        "    \"model_id\": \"prebuilt-document\",  # <FILL-ME: optional custom model id such as 'custom-invoice-v2'>\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Inject an authenticated Azure OpenAI client if you want the summariser to call the real service.\n",
        "# Example: from azure.identity import DefaultAzureCredential; from azure.ai.openai import AzureOpenAI\n",
        "#          AZURE_OPENAI_CLIENT = AzureOpenAI(credential=DefaultAzureCredential(), endpoint=\"https://<resource>.openai.azure.com\")\n",
        "AZURE_OPENAI_CLIENT = None  # <FILL-ME: Azure OpenAI client instance>\n",
        "AZURE_OPENAI_DEPLOYMENT = None  # <FILL-ME: deployment name, e.g. 'gpt-4o-mini'>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Replace this stub enrichment provider with the production integrations that conform to the EnrichmentProvider protocol.\n",
        "import re\n",
        "from typing import List, Optional\n",
        "\n",
        "from idp_service.enrichment import EnrichmentProvider, EnrichmentRequest, EnrichmentResponse\n",
        "\n",
        "\n",
        "class KeywordEnrichmentProvider(EnrichmentProvider):\n",
        "    name = \"keyword_insights\"\n",
        "    max_batch_size = 8\n",
        "    timeout_seconds: Optional[float] = 5.0\n",
        "\n",
        "    def enrich(self, requests: List[EnrichmentRequest]) -> List[EnrichmentResponse]:\n",
        "        responses: List[EnrichmentResponse] = []\n",
        "        pattern = re.compile(r\"[A-Za-z]{6,}\")\n",
        "        for request in requests:\n",
        "            text = \" \".join(span.content for span in request.document.text_spans)\n",
        "            keywords = sorted({word.lower() for word in pattern.findall(text)})[:10]\n",
        "            responses.append(\n",
        "                EnrichmentResponse(\n",
        "                    document_id=request.document_id,\n",
        "                    enrichments=[\n",
        "                        {\n",
        "                            \"enrichment_type\": \"keyword_summary\",\n",
        "                            \"content\": {\n",
        "                                \"keywords\": keywords,\n",
        "                                \"token_count\": len(text.split()),\n",
        "                            },\n",
        "                            \"confidence\": 0.55,\n",
        "                        }\n",
        "                    ],\n",
        "                    metadata={\"provider\": \"keyword_stub\"},\n",
        "                )\n",
        "            )\n",
        "        return responses\n",
        "\n",
        "\n",
        "ENRICHMENT_PROVIDERS = [KeywordEnrichmentProvider()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Rebuild rich sample documents\n",
        "\n",
        "The following cell recreates embedded documents and synthesises additional CSV/email assets so the routing logic encounters multiple content types.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import base64\n",
        "import json\n",
        "import textwrap\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from email.message import EmailMessage\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "from idp_service.sample_documents_embedded import write_embedded\n",
        "\n",
        "ARTIFACT_ROOT = Path(\"notebook_artifacts/production_simulation\")\n",
        "ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SAMPLE_PATHS: Dict[str, Path] = {}\n",
        "\n",
        "for name in (\"financial_report.pdf\", \"operating_budget.xlsx\"):\n",
        "    target = ARTIFACT_ROOT / name\n",
        "    write_embedded(name, target)\n",
        "    SAMPLE_PATHS[name] = target\n",
        "\n",
        "csv_content = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    customer_id,region,plan,revenue,renewal_probability\n",
        "    C-18273,EMEA,Enterprise,145000,0.82\n",
        "    C-19455,APAC,Professional,63000,0.71\n",
        "    C-20442,Americas,Enterprise,238000,0.94\n",
        "    C-22018,EMEA,SMB,27000,0.55\n",
        "    \"\"\"\n",
        ").strip()\n",
        "csv_path = ARTIFACT_ROOT / \"subscription_forecast.csv\"\n",
        "csv_path.write_text(csv_content, encoding=\"utf-8\")\n",
        "SAMPLE_PATHS[\"subscription_forecast.csv\"] = csv_path\n",
        "\n",
        "text_report = textwrap.dedent(\n",
        "    \"\"\"\n",
        "    Executive Summary\n",
        "\n",
        "    Our Q2 resilience initiatives drove faster onboarding times, while the incident response playbook reduced MTTR by 18%.\n",
        "    Customer sentiment remains positive; follow the appendix for next-step recommendations.\n",
        "    \"\"\"\n",
        ").strip()\n",
        "text_path = ARTIFACT_ROOT / \"executive_brief.txt\"\n",
        "text_path.write_text(text_report, encoding=\"utf-8\")\n",
        "SAMPLE_PATHS[\"executive_brief.txt\"] = text_path\n",
        "\n",
        "email_message = EmailMessage()\n",
        "email_message[\"Subject\"] = \"Compliance evidence package\"\n",
        "email_message[\"From\"] = \"ciso@example.com\"\n",
        "email_message[\"To\"] = \"auditor@example.com\"\n",
        "email_message[\"Date\"] = datetime.utcnow().strftime(\"%a, %d %b %Y %H:%M:%S +0000\")\n",
        "email_message.set_content(\n",
        "    textwrap.dedent(\n",
        "        \"\"\"\n",
        "        Team,\n",
        "\n",
        "        Please review the attached quarterly controls testing summary. The highlighted gaps require remediation sign-off before month-end.\n",
        "        Regards,\n",
        "        CISO\n",
        "        \"\"\"\n",
        "    ).strip()\n",
        ")\n",
        "email_message.add_alternative(\n",
        "    textwrap.dedent(\n",
        "        \"\"\"\n",
        "        <html>\n",
        "            <body>\n",
        "                <h2>Quarterly controls testing</h2>\n",
        "                <p>The SOC2 remediation tracker is attached. Key highlights:</p>\n",
        "                <ul>\n",
        "                    <li>87% of controls passed on first attempt.</li>\n",
        "                    <li>3 remediation items pending evidence.</li>\n",
        "                </ul>\n",
        "                <table>\n",
        "                    <tr><th>Control</th><th>Status</th><th>Owner</th></tr>\n",
        "                    <tr><td>Access Reviews</td><td>Pass</td><td>Security</td></tr>\n",
        "                    <tr><td>Change Management</td><td>Pass</td><td>Engineering</td></tr>\n",
        "                    <tr><td>Incident Response</td><td>Pending</td><td>Operations</td></tr>\n",
        "                </table>\n",
        "                <p>Regards,<br/>CISO</p>\n",
        "            </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "    ).strip(),\n",
        "    subtype=\"html\",\n",
        ")\n",
        "email_message.add_attachment(\n",
        "    SAMPLE_PATHS[\"financial_report.pdf\"].read_bytes(),\n",
        "    maintype=\"application\",\n",
        "    subtype=\"pdf\",\n",
        "    filename=\"financial_report.pdf\",\n",
        ")\n",
        "email_path = ARTIFACT_ROOT / \"compliance_package.eml\"\n",
        "email_path.write_bytes(email_message.as_bytes())\n",
        "SAMPLE_PATHS[\"compliance_package.eml\"] = email_path\n",
        "\n",
        "print(f\"Artifacts created in {ARTIFACT_ROOT.resolve()}\")\n",
        "for key, path in SAMPLE_PATHS.items():\n",
        "    print(f\"- {key}: {path.stat().st_size} bytes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional native dependencies\n",
        "\n",
        "For richer layout extraction (e.g., converting PDFs into structured text and reading XLSX tables), install `pymupdf` and `openpyxl` in your Databricks cluster before running the notebook. The LLM proxy falls back to heuristic parsers when these libraries are unavailable, which is why the summaries above contain raw binary snippets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Declare scenario matrix entries\n",
        "\n",
        "Each scenario mimics a production queue message, including S3 metadata, routing overrides, and enrichment hints. Modify or extend the entries to reflect your datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "SCENARIO_MATRIX: List[Dict[str, object]] = []\n",
        "\n",
        "def _layout_payload(page_count: int, *, text_density: float, image_density: float, table_density: float) -> Dict[str, object]:\n",
        "    return {\n",
        "        \"pageCount\": page_count,\n",
        "        \"layout\": {\n",
        "            \"textDensity\": text_density,\n",
        "            \"imageDensity\": image_density,\n",
        "            \"tableDensity\": table_density,\n",
        "        },\n",
        "    }\n",
        "\n",
        "def _register_scenario(\n",
        "    *,\n",
        "    name: str,\n",
        "    path: Path,\n",
        "    content_type: str,\n",
        "    layout: Dict[str, object],\n",
        "    enrichment_providers: Optional[List[str]] = None,\n",
        "    routing_overrides: Optional[Dict[str, object]] = None,\n",
        "    description: str,\n",
        ") -> None:\n",
        "    payload = path.read_bytes()\n",
        "    encoded = base64.b64encode(payload).decode(\"ascii\")\n",
        "    document_id = f\"{name}-{uuid.uuid4().hex[:8]}\"\n",
        "    object_key = f\"{name.replace('_', '/')}/{path.name}\"\n",
        "    body = {\n",
        "        \"documentMetadata\": {\n",
        "            \"contentType\": content_type,\n",
        "            **layout,\n",
        "        },\n",
        "        \"s3\": {\n",
        "            \"bucket\": {\"name\": \"idp-inbound-simulation\"},\n",
        "            \"object\": {\"key\": object_key},\n",
        "        },\n",
        "        \"documentBytes\": encoded,\n",
        "        \"enrichment\": {\"providers\": enrichment_providers or []},\n",
        "    }\n",
        "    if routing_overrides:\n",
        "        body[\"routing\"] = routing_overrides\n",
        "    SCENARIO_MATRIX.append(\n",
        "        {\n",
        "            \"scenario\": name,\n",
        "            \"description\": description,\n",
        "            \"document_id\": document_id,\n",
        "            \"object_key\": object_key,\n",
        "            \"body\": body,\n",
        "            \"document_bytes\": payload,\n",
        "            \"content_type\": content_type,\n",
        "            \"source_uri\": f\"s3://idp-inbound-simulation/{object_key}\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "_register_scenario(\n",
        "    name=\"long_form_financial_report\",\n",
        "    path=SAMPLE_PATHS[\"financial_report.pdf\"],\n",
        "    content_type=\"application/pdf\",\n",
        "    layout=_layout_payload(18, text_density=0.68, image_density=0.18, table_density=0.14),\n",
        "    enrichment_providers=[\"keyword_insights\"],\n",
        "    description=\"Multi-page financial PDF expected to follow the long-form route.\",\n",
        ")\n",
        "\n",
        "_register_scenario(\n",
        "    name=\"table_heavy_operating_budget\",\n",
        "    path=SAMPLE_PATHS[\"operating_budget.xlsx\"],\n",
        "    content_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n",
        "    layout=_layout_payload(4, text_density=0.35, image_density=0.05, table_density=0.9),\n",
        "    enrichment_providers=[\"keyword_insights\"],\n",
        "    description=\"Spreadsheet dominated by tabular data triggering table-heavy strategy.\",\n",
        ")\n",
        "\n",
        "_register_scenario(\n",
        "    name=\"subscription_forecast_csv\",\n",
        "    path=SAMPLE_PATHS[\"subscription_forecast.csv\"],\n",
        "    content_type=\"text/csv\",\n",
        "    layout=_layout_payload(1, text_density=0.4, image_density=0.0, table_density=0.85),\n",
        "    enrichment_providers=[\"keyword_insights\"],\n",
        "    description=\"CSV export treated as a table-heavy short document.\",\n",
        ")\n",
        "\n",
        "_register_scenario(\n",
        "    name=\"executive_brief_note\",\n",
        "    path=SAMPLE_PATHS[\"executive_brief.txt\"],\n",
        "    content_type=\"text/plain\",\n",
        "    layout=_layout_payload(2, text_density=0.82, image_density=0.05, table_density=0.1),\n",
        "    enrichment_providers=[\"keyword_insights\"],\n",
        "    description=\"Short memo expected to route through the short-form parser.\",\n",
        ")\n",
        "\n",
        "_register_scenario(\n",
        "    name=\"compliance_package_email\",\n",
        "    path=SAMPLE_PATHS[\"compliance_package.eml\"],\n",
        "    content_type=\"message/rfc822\",\n",
        "    layout=_layout_payload(3, text_density=0.6, image_density=0.15, table_density=0.25),\n",
        "    enrichment_providers=[\"keyword_insights\"],\n",
        "    routing_overrides={\"parser_override\": \"email_parser\"},\n",
        "    description=\"HTML-rich email forcing the email-specific parsing route via override.\",\n",
        ")\n",
        "\n",
        "print(f\"Registered {len(SCENARIO_MATRIX)} scenarios:\")\n",
        "for entry in SCENARIO_MATRIX:\n",
        "    print(f\"- {entry['scenario']}: {entry['description']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure routing with pattern overrides\n",
        "\n",
        "The router mirrors production behaviour: it uses heuristics to categorise each document, honours request-level overrides, and falls back to strategy defaults. Extend the override list with your own filename patterns or Delta-backed configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "from idp_service.routing import (\n",
        "    DocumentAnalysis,\n",
        "    DocumentCategory,\n",
        "    DocumentRouter,\n",
        "    HeuristicLayoutAnalyser,\n",
        "    OverrideSet,\n",
        "    PatternOverride,\n",
        "    RouterConfig,\n",
        "    RoutingMode,\n",
        "    StrategyConfig,\n",
        ")\n",
        "\n",
        "pattern_overrides = [\n",
        "    PatternOverride(\n",
        "        pattern=re.compile(r\"financial/.+\\.pdf$\", re.IGNORECASE),\n",
        "        strategy=StrategyConfig(name=\"custom_financial_parser\", model=\"finance-v1\"),\n",
        "    )\n",
        "]\n",
        "overrides = OverrideSet(pattern_overrides=pattern_overrides)\n",
        "\n",
        "default_strategy_map = {\n",
        "    DocumentCategory.SHORT_FORM.value: {\"name\": \"general_short_form\", \"model\": None},\n",
        "    DocumentCategory.LONG_FORM.value: {\"name\": \"custom_long_form\", \"model\": \"longform-v2\"},\n",
        "    DocumentCategory.TABLE_HEAVY.value: {\"name\": \"table_extractor\", \"model\": \"tabular-v2\"},\n",
        "    DocumentCategory.FORM_HEAVY.value: {\"name\": \"forms_extractor\", \"model\": \"forms-v1\"},\n",
        "    DocumentCategory.SCANNED.value: {\"name\": \"ocr_enhanced\", \"model\": \"ocr-2024\"},\n",
        "    DocumentCategory.UNKNOWN.value: {\"name\": \"fallback_non_azure\", \"model\": None},\n",
        "}\n",
        "\n",
        "router_config = RouterConfig(\n",
        "    mode=RoutingMode.HYBRID,\n",
        "    category_thresholds={\n",
        "        \"short_form_threshold\": 8,\n",
        "        \"long_form_threshold\": 25,\n",
        "        \"short_form_max_pages\": 6,\n",
        "        \"long_form_max_pages\": 120,\n",
        "        \"table_heavy_max_pages\": 12,\n",
        "        \"form_max_pages\": 10,\n",
        "    },\n",
        "    default_strategy_map=default_strategy_map,\n",
        "    fallback_strategy={\"name\": \"fallback_non_azure\", \"model\": None},\n",
        ")\n",
        "\n",
        "layout_analyser = HeuristicLayoutAnalyser()\n",
        "router = DocumentRouter(config=router_config, layout_analyser=layout_analyser)\n",
        "\n",
        "for entry in SCENARIO_MATRIX:\n",
        "    analysis: DocumentAnalysis = router.route(\n",
        "        body=entry[\"body\"],\n",
        "        object_key=entry[\"object_key\"],\n",
        "        overrides=overrides,\n",
        "    )\n",
        "    entry[\"analysis\"] = analysis\n",
        "    print(\n",
        "        f\"{entry['scenario']}: category={analysis.category.value}, \"\n",
        "        f\"strategy={analysis.strategy.name} (reason={analysis.strategy.reason})\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Assemble the Document Intelligence workflow\n",
        "\n",
        "This section builds the parsing workflow with either the LLM-based proxy or the real Azure Document Intelligence client, attaches the summariser, and wires in enrichment providers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from parsers.adapters import AzureDocumentIntelligenceAdapter\n",
        "from idp_service.document_intelligence_storage import InMemoryDocumentResultStore\n",
        "from idp_service.document_intelligence_workflow import DocumentIntelligenceWorkflow, WorkflowConfig\n",
        "from idp_service.llm_document_intelligence_proxy import LLMAzureDocumentIntelligenceClient\n",
        "from idp_service.summarization import DefaultDocumentSummarizer\n",
        "\n",
        "\n",
        "def build_document_intelligence_client():\n",
        "    if USE_LLM_PROXY_FOR_AZURE:\n",
        "        return LLMAzureDocumentIntelligenceClient()\n",
        "    try:\n",
        "        from azure.ai.formrecognizer import DocumentAnalysisClient\n",
        "        from azure.core.credentials import AzureKeyCredential\n",
        "    except ImportError as exc:  # pragma: no cover - informative guidance\n",
        "        raise RuntimeError(\n",
        "            \"Install azure-ai-formrecognizer and azure-core to use the real Azure Document Intelligence client\"\n",
        "        ) from exc\n",
        "\n",
        "    endpoint = AZURE_DOCUMENT_INTELLIGENCE_CONFIG[\"endpoint\"]\n",
        "    api_key = AZURE_DOCUMENT_INTELLIGENCE_CONFIG[\"api_key\"]\n",
        "    if not endpoint or \"<FILL-ME\" in endpoint:\n",
        "        raise RuntimeError(\"Provide a valid Azure Document Intelligence endpoint before disabling the proxy.\")\n",
        "    if not api_key or \"<FILL-ME\" in api_key:\n",
        "        raise RuntimeError(\"Provide a valid Azure Document Intelligence API key before disabling the proxy.\")\n",
        "\n",
        "    return DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(api_key))\n",
        "\n",
        "\n",
        "document_client = build_document_intelligence_client()\n",
        "adapter = AzureDocumentIntelligenceAdapter()\n",
        "result_store = InMemoryDocumentResultStore()\n",
        "\n",
        "summarizer = DefaultDocumentSummarizer(\n",
        "    azure_client=AZURE_OPENAI_CLIENT,\n",
        "    deployment_name=AZURE_OPENAI_DEPLOYMENT,\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "workflow_config = WorkflowConfig(\n",
        "    model_id=AZURE_DOCUMENT_INTELLIGENCE_CONFIG.get(\"model_id\") or \"prebuilt-document\",\n",
        "    adapter=adapter,\n",
        "    summarizer=summarizer,\n",
        "    enrichment_providers=ENRICHMENT_PROVIDERS,\n",
        ")\n",
        "\n",
        "workflow = DocumentIntelligenceWorkflow(\n",
        "    client=document_client,\n",
        "    store=result_store,\n",
        "    config=workflow_config,\n",
        ")\n",
        "\n",
        "workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Simulate asynchronous batch ingestion\n",
        "\n",
        "The helper functions below mimic the Databricks job draining SQS, routing documents, and dispatching them to worker tasks. Swap this logic with `idp_service.sqs_batch_ingestion` when you connect to live queues.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import base64\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Any, Dict, Iterable, List\n",
        "\n",
        "from parsers.canonical_schema import CanonicalDocument\n",
        "\n",
        "\n",
        "def _decode_document_bytes(entry: Dict[str, Any]) -> bytes:\n",
        "    payload = entry[\"body\"].get(\"documentBytes\")\n",
        "    if not isinstance(payload, str):\n",
        "        raise ValueError(\"Scenario entry missing base64-encoded documentBytes\")\n",
        "    return base64.b64decode(payload.encode(\"ascii\"))\n",
        "\n",
        "\n",
        "def process_document(entry: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    analysis: DocumentAnalysis = entry[\"analysis\"]\n",
        "    document_bytes = _decode_document_bytes(entry)\n",
        "    enrichment_requests = entry[\"body\"].get(\"enrichment\", {}).get(\"providers\", [])\n",
        "\n",
        "    metadata_record = analysis.to_metadata_record(\n",
        "        {\n",
        "            \"document_id\": entry[\"document_id\"],\n",
        "            \"object_key\": entry[\"object_key\"],\n",
        "            \"scenario\": entry[\"scenario\"],\n",
        "        }\n",
        "    )\n",
        "\n",
        "    workflow_result = workflow.process(\n",
        "        document_id=entry[\"document_id\"],\n",
        "        document_bytes=document_bytes,\n",
        "        source_uri=entry[\"source_uri\"],\n",
        "        metadata={\"routing\": metadata_record},\n",
        "        content_type=entry[\"content_type\"],\n",
        "        pages=None,\n",
        "        enrich_with=enrichment_requests,\n",
        "    )\n",
        "\n",
        "    canonical: CanonicalDocument = workflow_result.document  # type: ignore[assignment]\n",
        "    return {\n",
        "        \"scenario\": entry[\"scenario\"],\n",
        "        \"analysis\": analysis,\n",
        "        \"workflow_result\": workflow_result,\n",
        "        \"canonical\": canonical,\n",
        "    }\n",
        "\n",
        "\n",
        "def simulate_batches(entries: Iterable[Dict[str, Any]], batch_size: int = 2) -> List[Dict[str, Any]]:\n",
        "    entries = list(entries)\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    for start in range(0, len(entries), batch_size):\n",
        "        batch = entries[start : start + batch_size]\n",
        "        print(\n",
        "            f\"Dispatching batch {(start // batch_size) + 1} with {len(batch)} document(s)\"\n",
        "        )\n",
        "        with ThreadPoolExecutor(max_workers=len(batch)) as executor:\n",
        "            futures = [executor.submit(process_document, entry) for entry in batch]\n",
        "            for future in futures:\n",
        "                results.append(future.result())\n",
        "        time.sleep(0.1)  # mimic queue polling interval\n",
        "    return results\n",
        "\n",
        "\n",
        "SIMULATED_RESULTS = simulate_batches(SCENARIO_MATRIX, batch_size=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inspect canonical outputs, summaries, and enrichment payloads\n",
        "\n",
        "Review the structured payloads generated by the workflow. These records mirror what the production jobs persist to Delta Lake.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for result in SIMULATED_RESULTS:\n",
        "    canonical: CanonicalDocument = result[\"canonical\"]\n",
        "    print(\"=\" * 120)\n",
        "    print(f\"Scenario: {result['scenario']}\")\n",
        "    print(f\"Document ID: {canonical.document_id}\")\n",
        "    print(f\"Checksum: {canonical.checksum}\")\n",
        "    print(f\"Text spans: {len(canonical.text_spans)} | Tables: {len(canonical.tables)} | Fields: {len(canonical.fields)}\")\n",
        "    if canonical.summaries:\n",
        "        summary = canonical.summaries[-1]\n",
        "        print(f\"Summary: {summary.summary}\")\n",
        "        print(f\"Title: {summary.title} (confidence={summary.confidence})\")\n",
        "    else:\n",
        "        print(\"Summary: <none>\")\n",
        "    if canonical.enrichments:\n",
        "        print(\"Enrichment entries:\")\n",
        "        for enrichment in canonical.enrichments:\n",
        "            pprint(enrichment.to_dict())\n",
        "    else:\n",
        "        print(\"Enrichment entries: <none>\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Next steps for production deployment\n",
        "\n",
        "- Swap `simulate_batches` with the Databricks job runner in `idp_service.sqs_batch_ingestion` to process real SQS payloads.\n",
        "- Replace the keyword enrichment stub with your external service clients and enforce their response contract.\n",
        "- Point the `WorkflowConfig` at a `DeltaDocumentResultStore` to persist canonical documents to Delta Lake.\n",
        "- Schedule this notebook (or an exported Python script) as a Databricks job so operations teams can replay batches on demand.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}