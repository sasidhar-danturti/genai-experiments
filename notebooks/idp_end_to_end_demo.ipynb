{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Intelligent Document Processing End-to-End Demo\n\nThis notebook demonstrates the Databricks-based intelligent document processing (IDP) pipeline using Azure Document Intelligence abstractions. Instead of calling the Azure service directly, we rely on a lightweight LLM-backed proxy that emits the same schema as the Azure SDK so we can exercise routing, parsing, summarisation, enrichment, and asynchronous batching locally."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What you will see\n- Automatic routing based on document layout with optional overrides.\n- Parsing via an Azure Document Intelligence-compatible proxy that extracts text, tables, and fields.\n- Canonical normalisation with deterministic summaries and optional enrichment.\n- An asynchronous batching loop that mimics the SNS/SQS-driven orchestration."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Environment setup"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import importlib\nimport subprocess\nimport sys\n\npackages = [\n    ('fitz', 'pymupdf'),\n    ('openpyxl', 'openpyxl'),\n    ('fpdf', 'fpdf2'),\n]\nfor module_name, package_name in packages:\n    try:\n        importlib.import_module(module_name)\n    except Exception:\n        try:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', package_name])\n        except Exception as exc:  # noqa: BLE001\n            print(f'Package {package_name} is unavailable ({exc}); continuing with fallback paths.')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Imports and logging"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import base64\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import mimetypes\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "from urllib import request as urllib_request\n",
    "\n",
    "from idp_service.document_intelligence_storage import InMemoryDocumentResultStore\n",
    "from idp_service.document_intelligence_workflow import DocumentIntelligenceWorkflow, WorkflowConfig\n",
    "from idp_service.enrichment import EnrichmentProvider, EnrichmentRequest, EnrichmentResponse\n",
    "from idp_service.llm_document_intelligence_proxy import LLMAzureDocumentIntelligenceClient\n",
    "from idp_service.routing.router import (\n",
    "    DocumentRouter,\n",
    "    HeuristicLayoutAnalyser,\n",
    "    OverrideSet,\n",
    "    PatternOverride,\n",
    "    PyMuPDFLayoutAnalyser,\n",
    "    RouterConfig,\n",
    "    StrategyConfig,\n",
    "    DocumentAnalysis,\n",
    ")\n",
    "from parsers.adapters import AzureDocumentIntelligenceAdapter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Download or synthesise sample documents"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import mimetypes\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from urllib import request as urllib_request\n",
    "\n",
    "from idp_service.sample_documents_embedded import write_embedded\n",
    "\n",
    "DATA_DIR = Path('docs/sample_documents')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def download_file(url: str, target: Path) -> str:\n",
    "    try:\n",
    "        with urllib_request.urlopen(url) as response:\n",
    "            target.write_bytes(response.read())\n",
    "        return 'downloaded'\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        return f'fallback ({exc.__class__.__name__}: {exc})'\n",
    "\n",
    "\n",
    "def create_pdf(text: str, target: Path) -> bool:\n",
    "    try:\n",
    "        from fpdf import FPDF\n",
    "    except Exception:  # pragma: no cover - optional dependency\n",
    "        return False\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font('Helvetica', size=12)\n",
    "    for paragraph in textwrap.wrap(text, width=90):\n",
    "        pdf.multi_cell(0, 8, paragraph)\n",
    "    pdf.output(str(target))\n",
    "    return True\n",
    "\n",
    "\n",
    "def create_excel(target: Path) -> bool:\n",
    "    try:\n",
    "        from openpyxl import Workbook\n",
    "    except Exception:  # pragma: no cover - optional dependency\n",
    "        return False\n",
    "\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = 'Budget'\n",
    "    ws.append(['Category', 'Amount', 'Notes'])\n",
    "    ws.append(['Cloud spend', 25000, 'Azure + Databricks'])\n",
    "    ws.append(['Staff training', 8000, 'LLM, security, governance'])\n",
    "    ws.append(['Contingency', 5000, 'Reserved for overruns'])\n",
    "    wb.save(target)\n",
    "    return True\n",
    "\n",
    "\n",
    "def ensure_pdf(target: Path) -> str:\n",
    "    if target.exists():\n",
    "        return 'existing'\n",
    "    status = download_file('https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf', target)\n",
    "    if status == 'downloaded':\n",
    "        return status\n",
    "    if write_embedded('financial_report.pdf', target):\n",
    "        return 'embedded'\n",
    "    sample_text = (\n",
    "        'Contoso Retail consolidated financial summary. Revenue increased 18% year-on-year with strong growth in the '\n",
    "        'North region. Key initiatives include expanded e-commerce investments and cost optimisation in operations.'\n",
    "    )\n",
    "    if create_pdf(sample_text, target):\n",
    "        return 'generated'\n",
    "    raise RuntimeError('Unable to provision sample PDF; please provide one manually.')\n",
    "\n",
    "\n",
    "def ensure_csv(target: Path) -> str:\n",
    "    if target.exists():\n",
    "        return 'existing'\n",
    "    status = download_file('https://raw.githubusercontent.com/plotly/datasets/master/2014_usa_states.csv', target)\n",
    "    if status == 'downloaded':\n",
    "        return status\n",
    "    rows = [\n",
    "        ['Region', 'Product', 'Units', 'Revenue'],\n",
    "        ['North', 'Notebook', '120', '54000'],\n",
    "        ['West', 'Tablet', '75', '33750'],\n",
    "        ['South', 'Monitor', '55', '17600'],\n",
    "    ]\n",
    "    with target.open('w', newline='') as handle:\n",
    "        writer = csv.writer(handle)\n",
    "        writer.writerows(rows)\n",
    "    return 'generated'\n",
    "\n",
    "\n",
    "def ensure_excel(target: Path) -> str:\n",
    "    if target.exists():\n",
    "        return 'existing'\n",
    "    status = download_file('https://github.com/plotly/datasets/raw/master/2011_us_ag_exports.xlsx', target)\n",
    "    if status == 'downloaded':\n",
    "        return status\n",
    "    if write_embedded('operating_budget.xlsx', target):\n",
    "        return 'embedded'\n",
    "    if create_excel(target):\n",
    "        return 'generated'\n",
    "    raise RuntimeError('Unable to provision sample XLSX; please provide one manually.')\n",
    "\n",
    "\n",
    "pdf_path = DATA_DIR / 'financial_report.pdf'\n",
    "csv_path = DATA_DIR / 'quarterly_sales.csv'\n",
    "xlsx_path = DATA_DIR / 'operating_budget.xlsx'\n",
    "email_path = DATA_DIR / 'customer_email.eml'\n",
    "\n",
    "pdf_status = ensure_pdf(pdf_path)\n",
    "csv_status = ensure_csv(csv_path)\n",
    "xlsx_status = ensure_excel(xlsx_path)\n",
    "\n",
    "if not email_path.exists():\n",
    "    from email.message import EmailMessage\n",
    "\n",
    "    message = EmailMessage()\n",
    "    message['Subject'] = 'Quarterly performance review'\n",
    "    message['From'] = 'finance.lead@example.com'\n",
    "    message['To'] = 'cfo@example.com'\n",
    "    body = (\n",
    "        'Hi, attached are the revenue extracts and the updated operating budget. '\n",
    "        'Please review before the leadership sync. The north region still leads growth.'\n",
    "    )\n",
    "    message.set_content(body)\n",
    "    for attachment_path in (csv_path, xlsx_path):\n",
    "        data = attachment_path.read_bytes()\n",
    "        mime_type, _ = mimetypes.guess_type(attachment_path.name)\n",
    "        maintype, subtype = (mime_type or 'application/octet-stream').split('/', 1)\n",
    "        message.add_attachment(data, maintype=maintype, subtype=subtype, filename=attachment_path.name)\n",
    "    email_path.write_bytes(message.as_bytes())\n",
    "    email_status = 'generated'\n",
    "else:\n",
    "    email_status = 'existing'\n",
    "\n",
    "print('PDF source:', pdf_status)\n",
    "print('CSV source:', csv_status)\n",
    "print('Excel source:', xlsx_status)\n",
    "print('Email status:', email_status)\n",
    "\n",
    "sample_assets = {\n",
    "    'pdf': pdf_path,\n",
    "    'csv': csv_path,\n",
    "    'xlsx': xlsx_path,\n",
    "    'email': email_path,\n",
    "}\n",
    "sample_assets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Configure the router"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "router_config = RouterConfig(\n    default_strategy_map={\n        'short_form': {'name': 'prebuilt-layout'},\n        'long_form': {'name': 'custom-long-form'},\n        'table_heavy': {'name': 'prebuilt-table'},\n        'form_heavy': {'name': 'prebuilt-form'},\n        'unknown': {'name': 'prebuilt-layout'},\n    },\n    fallback_strategy={'name': 'prebuilt-layout'},\n)\n\ntry:\n    layout_analyser = PyMuPDFLayoutAnalyser()\nexcept Exception:  # noqa: BLE001\n    layout_analyser = HeuristicLayoutAnalyser()\n\noverride_set = OverrideSet(\n    pattern_overrides=[\n        PatternOverride(pattern=re.compile(r'.*\\.csv$'), strategy=StrategyConfig(name='prebuilt-table')),\n        PatternOverride(pattern=re.compile(r'.*\\.xlsx$'), strategy=StrategyConfig(name='prebuilt-table')),\n    ]\n)\n\nrouter = DocumentRouter(router_config, layout_analyser)\nrouter_config, override_set\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Instantiate the parsing workflow and enrichment provider"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "proxy_client = LLMAzureDocumentIntelligenceClient()\nadapter = AzureDocumentIntelligenceAdapter()\nresult_store = InMemoryDocumentResultStore()\n\nclass KeywordEnrichmentProvider:\n    name = 'keyword_insights'\n    max_batch_size = 8\n    timeout_seconds: Optional[float] = 5.0\n\n    def enrich(self, requests: List[EnrichmentRequest]) -> List[EnrichmentResponse]:\n        responses: List[EnrichmentResponse] = []\n        for request in requests:\n            text = ' '.join(span.content for span in request.document.text_spans)\n            keywords = sorted({word.lower() for word in re.findall(r'[A-Za-z]{6,}', text)})[:6]\n            enrichments = [\n                {\n                    'enrichment_type': 'keyword_summary',\n                    'content': {\n                        'keywords': keywords,\n                        'token_count': len(text.split()),\n                    },\n                }\n            ]\n            responses.append(\n                EnrichmentResponse(\n                    document_id=request.document_id,\n                    enrichments=enrichments,\n                    metadata={'provider': 'keyword'},\n                )\n            )\n        return responses\n\nkeyword_provider = KeywordEnrichmentProvider()\nkeyword_provider.name, keyword_provider.max_batch_size\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Describe document jobs"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "@dataclass\nclass DocumentJob:\n    document_id: str\n    label: str\n    path: Path\n    metadata: Dict[str, str]\n    parser_override: Optional[str] = None\n    enrich_with: Optional[List[str]] = None\n    force: bool = False\n\njobs: List[DocumentJob] = [\n    DocumentJob(\n        document_id='financial-report',\n        label='Financial report PDF',\n        path=sample_assets['pdf'],\n        metadata={\n            'mime_type': 'application/pdf',\n            'document_type': 'financial_report',\n            'object_key': sample_assets['pdf'].name,\n        },\n        enrich_with=['keyword_insights'],\n    ),\n    DocumentJob(\n        document_id='quarterly-sales',\n        label='Sales CSV baseline',\n        path=sample_assets['csv'],\n        metadata={\n            'mime_type': 'text/csv',\n            'document_type': 'sales_data',\n            'object_key': sample_assets['csv'].name,\n        },\n        enrich_with=['keyword_insights'],\n    ),\n    DocumentJob(\n        document_id='quarterly-sales',\n        label='Sales CSV duplicate without force',\n        path=sample_assets['csv'],\n        metadata={\n            'mime_type': 'text/csv',\n            'document_type': 'sales_data',\n            'object_key': sample_assets['csv'].name,\n        },\n        enrich_with=['keyword_insights'],\n    ),\n    DocumentJob(\n        document_id='quarterly-sales',\n        label='Sales CSV override with force',\n        path=sample_assets['csv'],\n        metadata={\n            'mime_type': 'text/csv',\n            'document_type': 'sales_data',\n            'object_key': sample_assets['csv'].name,\n        },\n        parser_override='prebuilt-layout',\n        enrich_with=['keyword_insights'],\n        force=True,\n    ),\n    DocumentJob(\n        document_id='operating-budget',\n        label='Operating budget spreadsheet',\n        path=sample_assets['xlsx'],\n        metadata={\n            'mime_type': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n            'document_type': 'budget',\n            'object_key': sample_assets['xlsx'].name,\n        },\n        enrich_with=['keyword_insights'],\n    ),\n    DocumentJob(\n        document_id='customer-email',\n        label='Customer email with attachments',\n        path=sample_assets['email'],\n        metadata={\n            'mime_type': 'message/rfc822',\n            'document_type': 'email',\n            'object_key': sample_assets['email'].name,\n        },\n        enrich_with=['keyword_insights'],\n    ),\n]\nlen(jobs)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Helper functions for routing and processing"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def build_router_body(job: DocumentJob, document_bytes: bytes) -> Dict[str, object]:\n    payload = {\n        'documentBytes': base64.b64encode(document_bytes).decode('ascii'),\n        'documentMetadata': {\n            'mimeType': job.metadata.get('mime_type'),\n            'documentType': job.metadata.get('document_type'),\n        },\n    }\n    if job.parser_override:\n        payload['parser_override'] = job.parser_override\n    return payload\n\ndef route_document(job: DocumentJob) -> tuple[bytes, DocumentAnalysis]:\n    document_bytes = job.path.read_bytes()\n    body = build_router_body(job, document_bytes)\n    analysis = router.route(body, job.metadata['object_key'], override_set)\n    return document_bytes, analysis\n\ndef run_workflow(job: DocumentJob, document_bytes: bytes, analysis: DocumentAnalysis):\n    metadata = {\n        'document_type': job.metadata.get('document_type'),\n        'mime_type': job.metadata.get('mime_type'),\n        'routing': analysis.to_metadata_record({'document_id': job.document_id}),\n    }\n    workflow = DocumentIntelligenceWorkflow(\n        client=proxy_client,\n        store=result_store,\n        config=WorkflowConfig(\n            model_id=analysis.strategy.name,\n            adapter=adapter,\n            enrichment_providers=(keyword_provider,),\n        ),\n    )\n    return workflow.process(\n        document_id=job.document_id,\n        document_bytes=document_bytes,\n        source_uri=str(job.path),\n        metadata=metadata,\n        content_type=job.metadata.get('mime_type'),\n        force=job.force,\n        enrich_with=job.enrich_with,\n    )\n\nasync def process_jobs_async(job_list: List[DocumentJob], worker_count: int = 3):\n    queue: asyncio.Queue[Optional[DocumentJob]] = asyncio.Queue()\n    for job in job_list:\n        await queue.put(job)\n    for _ in range(worker_count):\n        await queue.put(None)\n\n    events: List[Dict[str, object]] = []\n    final_documents: Dict[str, object] = {}\n\n    async def worker(name: str) -> None:\n        while True:\n            job = await queue.get()\n            if job is None:\n                queue.task_done()\n                break\n            document_bytes, analysis = await asyncio.to_thread(route_document, job)\n            workflow_result = await asyncio.to_thread(run_workflow, job, document_bytes, analysis)\n            events.append(\n                {\n                    'worker': name,\n                    'job': job,\n                    'analysis': analysis,\n                    'workflow_result': workflow_result,\n                }\n            )\n            if not workflow_result.skipped and workflow_result.document is not None:\n                final_documents[job.document_id] = workflow_result.document\n            queue.task_done()\n\n    workers = [asyncio.create_task(worker(f'worker-{idx+1}')) for idx in range(worker_count)]\n    await queue.join()\n    for worker_task in workers:\n        await worker_task\n    return events, final_documents\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Run the asynchronous pipeline"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "events, final_documents = asyncio.run(process_jobs_async(jobs, worker_count=3))\nlen(events), list(final_documents.keys())\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Inspect routing decisions and canonical outputs"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def _normalise_adi_result(raw: Any) -> Any:\n",
    "    if raw is None:\n",
    "        return None\n",
    "    if isinstance(raw, dict):\n",
    "        return raw\n",
    "    for attr in (\"to_dict\", \"as_dict\"):\n",
    "        method = getattr(raw, attr, None)\n",
    "        if callable(method):\n",
    "            try:\n",
    "                return method()\n",
    "            except Exception:\n",
    "                pass\n",
    "    if hasattr(raw, \"__dict__\"):\n",
    "        try:\n",
    "            return {k: v for k, v in raw.__dict__.items() if not k.startswith(\"_\")}\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {\"repr\": repr(raw)}\n",
    "\n",
    "def describe_event(event: Dict[str, object]) -> Dict[str, object]:\n",
    "    job: DocumentJob = event['job']  # type: ignore[assignment]\n",
    "    analysis: DocumentAnalysis = event['analysis']  # type: ignore[assignment]\n",
    "    workflow_result = event['workflow_result']\n",
    "    routing_record = analysis.to_metadata_record({'document_id': job.document_id})\n",
    "\n",
    "    document_summary = None\n",
    "    enrichment_summary = None\n",
    "    canonical_payload = None\n",
    "    denorm_records = []\n",
    "    attachments = None\n",
    "\n",
    "    if workflow_result.document is not None:\n",
    "        doc = workflow_result.document\n",
    "        canonical_payload = doc.to_dict()\n",
    "        document_summary = {\n",
    "            'title': doc.summaries[0].title if doc.summaries else None,\n",
    "            'summary': doc.summaries[0].summary if doc.summaries else None,\n",
    "            'text_span_count': len(doc.text_spans),\n",
    "            'table_count': len(doc.tables),\n",
    "        }\n",
    "        enrichment_summary = [enrichment.to_dict() for enrichment in doc.enrichments]\n",
    "        if doc.attachments:\n",
    "            attachments = [\n",
    "                {\n",
    "                    'attachment_id': attachment.attachment_id,\n",
    "                    'file_name': attachment.file_name,\n",
    "                    'mime_type': attachment.mime_type,\n",
    "                    'document_id': attachment.document.document_id if attachment.document else None,\n",
    "                }\n",
    "                for attachment in doc.attachments\n",
    "            ]\n",
    "        denorm_records = [record.to_dict() for record in workflow_result.records]\n",
    "\n",
    "    adi_result = _normalise_adi_result(getattr(workflow_result, 'raw_result', None))\n",
    "\n",
    "    return {\n",
    "        'label': job.label,\n",
    "        'document_id': job.document_id,\n",
    "        'strategy': analysis.strategy.name,\n",
    "        'strategy_reason': analysis.strategy.reason,\n",
    "        'category': analysis.category.value,\n",
    "        'overrides_applied': analysis.overrides_applied,\n",
    "        'skipped': workflow_result.skipped,\n",
    "        'summary': document_summary,\n",
    "        'enrichments': enrichment_summary,\n",
    "        'routing_metadata': routing_record,\n",
    "        'adi_result': adi_result,\n",
    "        'canonical_document': canonical_payload,\n",
    "        'denormalized_records': denorm_records,\n",
    "        'attachments': attachments,\n",
    "    }\n",
    "\n",
    "event_summaries = [describe_event(event) for event in events]\n",
    "event_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Normalised payloads ready for downstream consumption"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "canonical_payloads = {doc_id: doc.to_dict() for doc_id, doc in final_documents.items()}\njson.dumps(canonical_payloads, indent=2)[:2000] + '...'\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The JSON snippet above shows the canonical schema with text spans, tables, fields, summaries, and enrichment outputs. Downstream services can rely on this standard contract regardless of which parser handled the raw document."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
